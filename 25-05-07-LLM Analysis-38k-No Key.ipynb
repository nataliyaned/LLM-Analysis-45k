{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51387400",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628c114",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8ba6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3084ea7",
   "metadata": {},
   "source": [
    "## 1. Signing up for API access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa0750",
   "metadata": {},
   "source": [
    "The first step is to sign up to API access with OpenAI. This can be done on platform.openai.com. (See the PDF how-to guide for further instructions.)\n",
    "\n",
    "You will receive an API key to be used below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6fbd8",
   "metadata": {},
   "source": [
    "## 2. Installing and loading the relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351fd23",
   "metadata": {},
   "source": [
    "We first need to install and import the relevant libraries: the pandas package for general data processing, and the openai package for interacting with the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f94ae26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/Nataliya/Desktop/Sentiment Analysis',\n",
       " '/Users/Nataliya/anaconda3/lib/python311.zip',\n",
       " '/Users/Nataliya/anaconda3/lib/python3.11',\n",
       " '/Users/Nataliya/anaconda3/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/Users/Nataliya/anaconda3/lib/python3.11/site-packages',\n",
       " '/Users/Nataliya/anaconda3/lib/python3.11/site-packages/aeosa']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# which python it's running\n",
    "sys.executable\n",
    "\n",
    "# where it's looking for imports\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d261c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: openai in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from openai) (1.10.8)\n",
      "Requirement already satisfied: sniffio in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: numpy in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (1.26.2)\n"
     ]
    }
   ],
   "source": [
    "#Install the libraries\n",
    "!pip install pandas\n",
    "!pip install openai\n",
    "!pip install numpy\n",
    "#!pip install numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc8d060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Call the libraries\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "#Set the API key. See the how-to guide for furhter instructions\n",
    "#openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eaffb53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model answer: 'The meaning of life is a deeply philosophical question that has been explored by thinkers, theologians, and individuals throughout history. Answers can vary widely based on cultural, religious, personal, and existential perspectives. \n",
      "\n",
      "1. **Philosophical Views**: Some philosophers suggest that life's meaning is derived from individual experiences, relationships, and contributions to society. Others argue for an existential approach, where meaning is not given but created by individuals through their choices and actions.\n",
      "\n",
      "2. **Religious Perspectives**: Many religions provide frameworks for understanding life's purpose, often focusing on concepts like worship, moral conduct, community'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "# personal:\n",
    "organization=''\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the meaning of life?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\", max_tokens=120\n",
    ")\n",
    "\n",
    "# deprecated:\n",
    "#response = openai.ChatCompletion.create(\n",
    "#  model = 'gpt-4', #Which model to use\n",
    "#  temperature=0.2, #How random is the answer\n",
    "#  max_tokens=120, #How long can the reply be\n",
    "#  messages=[\n",
    "#    {\"role\": \"user\", \n",
    "#    \"content\": \"What is the meaning of life?\"}]\n",
    "# )\n",
    "\n",
    "result = ''\n",
    "for choice in chat_completion.choices:\n",
    "    result += choice.message.content\n",
    "print(f\"Model answer: '{result}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8c0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define which model to use throughout\n",
    "MODEL = 'gpt-4o-mini'\n",
    "MAX_TOKENS = 4000\n",
    "WAIT_TIME = 0.8 # Wait time between each request. This depends on the rate limit of the model used: GPT-4 needs longer wait time than GPT-3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debacd0e",
   "metadata": {},
   "source": [
    "We can now call the OpenAI API. For instance, we can ask ChatGPT-4 a question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f2054",
   "metadata": {},
   "source": [
    "If the code above generates an error, you might need to check whether your API key is correct, and whether you have access to the specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de402e",
   "metadata": {},
   "source": [
    "## 3. Loading and preparing your test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899405a",
   "metadata": {},
   "source": [
    "The next step is to load and prepare the data that we want to analyze. We will load the data into a Pandas dataframe to allow easy processing.\n",
    "\n",
    "The details of how to open your particular data depends on the structure and format of the data. Pandas offers ways of opening a range of file formats, including CSV and Excel files. You may wish to refer to the Pandas documentation for more details.\n",
    "\n",
    "In our example, we will use the data from the Global Populism Dataset (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/LFTQEZ). This data offers a number of texts from politicians, and can be used for validating our method. The texts are provided as .txt files in a folder. We will load all these files into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2439d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data from textfiles\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "# Define the folder path where the text files are located\n",
    "#folder_path = './NYT Test/'\n",
    "\n",
    "# Use glob to get a list of all *.txt files in the folder\n",
    "#txt_files = glob.glob(folder_path + '/*.txt')\n",
    "\n",
    "# Process for moving from txt files to data frame\n",
    "# Create an empty list to store the data\n",
    "# data = []\n",
    "\n",
    "# # Loop through each text file\n",
    "# for file_path in txt_files:\n",
    "#     with open(file_path, 'r',encoding='utf-8',errors='ignore') as file:\n",
    "#         # Read the text from the file\n",
    "#         text = file.read()\n",
    "\n",
    "#         # Get the filename without the directory path\n",
    "#         filename = os.path.basename(file_path)\n",
    "\n",
    "#         # Append the text and filename to the data list\n",
    "#         data.append({'filename': filename, 'text': text})\n",
    "\n",
    "# Create a dataframe with the data\n",
    "\n",
    "df= pd.read_csv(\"/Users/Nataliya/Desktop/mergev3.csv\")\n",
    "\n",
    "#df = pd.DataFrame(data['summary_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1211733e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X</th>\n",
       "      <th>year</th>\n",
       "      <th>V1</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>header</th>\n",
       "      <th>source</th>\n",
       "      <th>byline</th>\n",
       "      <th>date</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>copyrightco</th>\n",
       "      <th>length</th>\n",
       "      <th>photo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>\"_AFFORDABLE WEAPON_ TESTS UNSUCCESSFUL, BUT H...</td>\n",
       "      <td>Despite unsuccessful tests a weapon designed t...</td>\n",
       "      <td>Vol. 19; No. 21</td>\n",
       "      <td>\"_AFFORDABLE WEAPON_ TESTS UNSUCCESSFUL, BUT H...</td>\n",
       "      <td>Inside the Navy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May 29, 2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006 Inside Washington Publishers</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2006</td>\n",
       "      <td>\"_At this point no new talks, no new negotiati...</td>\n",
       "      <td>Oct. 7--ST. MARYS -- Picketers hit the line in...</td>\n",
       "      <td>STATE AND REGIONAL NEWS</td>\n",
       "      <td>\"_At this point no new talks, no new negotiati...</td>\n",
       "      <td>The Lima News (Ohio)</td>\n",
       "      <td>Bob Blake, The Lima News, OhioB</td>\n",
       "      <td>October 7, 2006 Saturday</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>504.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>\"_Can_t Imagine a Better Time_ - One on One Wi...</td>\n",
       "      <td>In his first interview as Sikorsky president J...</td>\n",
       "      <td>Vol. 40; No. 8</td>\n",
       "      <td>\"_Can_t Imagine a Better Time_ - One on One Wi...</td>\n",
       "      <td>Rotor &amp; Wing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August 15, 2006 Tuesday</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2006 Access Intelligence, LLC</td>\n",
       "      <td>2578.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>\"_Chaotic_ scene as carpenters walk Despite _p...</td>\n",
       "      <td>About 5000 unionized carpenters walked off the...</td>\n",
       "      <td>LOCAL; Pg. 07</td>\n",
       "      <td>\"_Chaotic_ scene as carpenters walk Despite _p...</td>\n",
       "      <td>Philadelphia Daily News</td>\n",
       "      <td>DAVE DAVIES, B</td>\n",
       "      <td>May 2, 2006 Tuesday</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2006 Philadelphia Newspapers, LLC</td>\n",
       "      <td>380.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>\"_I just didn_t think this was going to happen...</td>\n",
       "      <td>Retiree worried about benefits helps staff the...</td>\n",
       "      <td>A; Pg. 1</td>\n",
       "      <td>\"_I just didn_t think this was going to happen...</td>\n",
       "      <td>Lincoln Journal Star (Nebraska)</td>\n",
       "      <td>COLLEEN KENNEY, Lincoln Journal StarDateline:L...</td>\n",
       "      <td>October 12, 2006 ThursdayCity Edition</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2006 Lincoln Journal Star,</td>\n",
       "      <td>883.0</td>\n",
       "      <td>_I just didn_t think this was going to happen_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  X  year                                                 V1  \\\n",
       "0           1  1  2006  \"_AFFORDABLE WEAPON_ TESTS UNSUCCESSFUL, BUT H...   \n",
       "1           2  3  2006  \"_At this point no new talks, no new negotiati...   \n",
       "2           3  4  2006  \"_Can_t Imagine a Better Time_ - One on One Wi...   \n",
       "3           4  6  2006  \"_Chaotic_ scene as carpenters walk Despite _p...   \n",
       "4           5  8  2006  \"_I just didn_t think this was going to happen...   \n",
       "\n",
       "                                             summary                    title  \\\n",
       "0  Despite unsuccessful tests a weapon designed t...          Vol. 19; No. 21   \n",
       "1  Oct. 7--ST. MARYS -- Picketers hit the line in...  STATE AND REGIONAL NEWS   \n",
       "2  In his first interview as Sikorsky president J...           Vol. 40; No. 8   \n",
       "3  About 5000 unionized carpenters walked off the...            LOCAL; Pg. 07   \n",
       "4  Retiree worried about benefits helps staff the...                 A; Pg. 1   \n",
       "\n",
       "                                              header  \\\n",
       "0  \"_AFFORDABLE WEAPON_ TESTS UNSUCCESSFUL, BUT H...   \n",
       "1  \"_At this point no new talks, no new negotiati...   \n",
       "2  \"_Can_t Imagine a Better Time_ - One on One Wi...   \n",
       "3  \"_Chaotic_ scene as carpenters walk Despite _p...   \n",
       "4  \"_I just didn_t think this was going to happen...   \n",
       "\n",
       "                            source  \\\n",
       "0                  Inside the Navy   \n",
       "1             The Lima News (Ohio)   \n",
       "2                     Rotor & Wing   \n",
       "3          Philadelphia Daily News   \n",
       "4  Lincoln Journal Star (Nebraska)   \n",
       "\n",
       "                                              byline  \\\n",
       "0                                                NaN   \n",
       "1                    Bob Blake, The Lima News, OhioB   \n",
       "2                                                NaN   \n",
       "3                                     DAVE DAVIES, B   \n",
       "4  COLLEEN KENNEY, Lincoln Journal StarDateline:L...   \n",
       "\n",
       "                                    date dayofweek  \\\n",
       "0                           May 29, 2006       NaN   \n",
       "1               October 7, 2006 Saturday  Saturday   \n",
       "2                August 15, 2006 Tuesday   Tuesday   \n",
       "3                    May 2, 2006 Tuesday   Tuesday   \n",
       "4  October 12, 2006 ThursdayCity Edition  Thursday   \n",
       "\n",
       "                           copyrightco  length  \\\n",
       "0   2006 Inside Washington Publishers   1207.0   \n",
       "1                                  NaN   504.0   \n",
       "2       2006 Access Intelligence, LLC   2578.0   \n",
       "3   2006 Philadelphia Newspapers, LLC    380.0   \n",
       "4          2006 Lincoln Journal Star,    883.0   \n",
       "\n",
       "                                            photo  \n",
       "0                                             NaN  \n",
       "1                                             NaN  \n",
       "2                                             NaN  \n",
       "3                                             NaN  \n",
       "4  _I just didn_t think this was going to happen_  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d9991",
   "metadata": {},
   "source": [
    "### Filter the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6261989",
   "metadata": {},
   "source": [
    "You will likely need to filter out and select the data you wish to include. \n",
    "\n",
    "In our case, we will filter out texts with non-latin alphabets. While ChatGPT can handle languages with non-latin characters, it is currently more expensive, and there are issues with managing text length. For simplicity, we therefore remove the texts with non-latin alphabet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ba88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filtering out special tokens\n",
    "\n",
    "#import re\n",
    "\n",
    "#def sanitize_text(text):\n",
    "    # Check if the value is a float\n",
    "#    if isinstance(text, float):\n",
    "#        return str(text)\n",
    "\n",
    "    # Define a regular expression pattern to match allowed characters\n",
    "#    allowed_pattern = re.compile(r'[a-zA-Z0-9\\s.,?!-]')\n",
    "\n",
    "    # Use the pattern to filter out disallowed characters\n",
    "#    sanitized_text = ''.join(filter(allowed_pattern.match, str(text)))\n",
    "\n",
    "#    return sanitized_text\n",
    "\n",
    "# Apply the sanitize_text function to the 'summary_final' column\n",
    "#df['summary_and_photo'] = df['summary_and_photo'].apply(sanitize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c1dd87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def is_latin_alphabet(text):\n",
    "#     latin_characters = 0\n",
    "#     total_characters = 0\n",
    "\n",
    "#     for char in text:\n",
    "#         if ord(char) >= 0x0000 and ord(char) <= 0x007F:\n",
    "#             latin_characters += 1\n",
    "#         total_characters += 1\n",
    "\n",
    "#     # Check if the majority of characters are Latin alphabet characters\n",
    "#     if latin_characters / total_characters >= 0.9:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "# df = df[df['summary_and_photo'].apply(is_latin_alphabet)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ac6f7",
   "metadata": {},
   "source": [
    "### Chunking the texts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fc70e",
   "metadata": {},
   "source": [
    "Unlike other NLP methods, not much preprocessing is needed. However, LLMs are only able to process texts that are smaller than their \"context window\". If our texts are longer than the context window of our model, we have to either split the texts into several smaller chunks and analyze them part by part, or simply truncate the text (not recommended).\n",
    "\n",
    "The details depend on the model you use and the amount for data. For our example, with the GPT-4-32k model, our speeches all fit in the model window, and we do not need to split the texts. \n",
    "\n",
    "However, for pedagogical reasons, we will use the standard 8K GPT-4 model and chunk the text into smaller pieces. If your text is short, such as a tweet, this function will do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b20c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to chunk the text into pieces, separated on sentence level.\n",
    "# To do so, we use the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "956905f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Requirement already satisfied: nltk in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: langdetect in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/Nataliya/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "!pip install nltk\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c46beddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nataliya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken \n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22826413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code chunks the text into processable pieces of similar size.\n",
    "#If the text is longer than allowed in terms of model tokens, we want to split the text into equally sized parts, without splitting any text mid-sentence.\n",
    "def split_text_into_chunks(text, max_tokens):\n",
    "    \n",
    "    #Code the text in gpt coding and calculate the number of tokens\n",
    "    encoding = tiktoken.encoding_for_model(MODEL)\n",
    "    nrtokens = len(encoding.encode(text))\n",
    "        \n",
    "    if nrtokens < max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    #how many chunks to split it into?\n",
    "    num_chunks = np.ceil(nrtokens / max_tokens)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Calculate the number of words per chunk\n",
    "    words_per_chunk = len(text.split()) // num_chunks\n",
    "\n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    word_counter = 0\n",
    "    # Iterate through each sentence\n",
    "    for sentence in sentences:\n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        word_counter += len(sentence.split())\n",
    "\n",
    "        # Check if the current chunk has reached the desired number of words\n",
    "        if word_counter >= words_per_chunk:\n",
    "            # Add the current chunk to the list of chunks\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            word_counter = 0\n",
    "            # Reset the current chunk\n",
    "            current_chunk = []\n",
    "\n",
    "    # Add the remaining sentences as the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88198655",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "\n",
    "# def split_text_into_chunks(text, max_tokens):\n",
    "#     # Check if the input is a string\n",
    "#     if not isinstance(text, str):\n",
    "#         return []\n",
    "\n",
    "#     # Code the text in GPT coding and calculate the number of tokens\n",
    "#     encoding = tiktoken.encoding_for_model(MODEL)\n",
    "#     nrtokens = len(encoding.encode(text))\n",
    "\n",
    "#     # If the number of tokens is less than max_tokens, return the text as a single chunk\n",
    "#     if nrtokens < max_tokens:\n",
    "#         return [text]\n",
    "\n",
    "#     # Split the text into chunks\n",
    "#     chunks = []\n",
    "#     current_chunk = \"\"\n",
    "#     for token in encoding.tokenize(text):\n",
    "#         if len(encoding.encode(current_chunk + token)) <= max_tokens:\n",
    "#             current_chunk += token\n",
    "#         else:\n",
    "#             chunks.append(current_chunk)\n",
    "#             current_chunk = token\n",
    "\n",
    "#     # Add the last chunk\n",
    "#     if current_chunk:\n",
    "#         chunks.append(current_chunk)\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# # Maximum number of words per chunk, this depends on the model context window.\n",
    "# # Set it to a bit lower than the max tokens to leave space for instruction and response.\n",
    "# max_tokens = MAX_TOKENS - 2000\n",
    "\n",
    "# # Apply the split_text_into_chunks function to the 'summary_final' column\n",
    "# df['text_chunks'] = df['summary_and_photo'].apply(lambda x: split_text_into_chunks(x, max_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eee3275e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the 'summary' column with empty strings\n",
    "df['summary'] = df['summary'].fillna('')\n",
    "\n",
    "# Convert non-string values to strings\n",
    "df['summary'] = df['summary'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcaf450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to an earlier version, otherwise tiktoken cannot find a tokenizer. Doesn't matter because 3.5 and 4.0 use the same tokenizer\n",
    "MODEL = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb35e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of words per chunk, this depends on the model context window. \n",
    "# We set it to a bit lower than the max tokens, to leave space for our instruction and the response.\n",
    "\n",
    "max_tokens = MAX_TOKENS - 200\n",
    "df['text_chunks'] = df['summary'].apply(lambda x: split_text_into_chunks(x, max_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0fa233",
   "metadata": {},
   "source": [
    "# 4. Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a78dfc",
   "metadata": {},
   "source": [
    "The next step is to formulate a first instructions for analyzing the text. The prompts will be a result of an iterative process through which you develop a formulation of the concept that you wish to capture. \n",
    "\n",
    "We here start by drawing on the instructions for human coders from a previous study.\n",
    "\n",
    "See the how-to guide for details on this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e886a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"instruction = Evaluate a news article covering a worker protest and assign two numeric grades in the folllowing JSON format and place into an empty column in the dataset. Replace the NUMERIC GRADE placeholders with the actual numeric grades and the NUMERIC GRADE EXPLANATION placeholders with the actual numeric grade explanations. Do not add any other tokens:\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"grades\": [\n",
    "      {\n",
    "        \"grade1\": 0,\n",
    "        \"numericGrade1\": \"0\",\n",
    "        \"numericGrade1explain\": \"The article does not mention a labor strike or workplace protest, but discusses a political decision regarding media.\"\n",
    "      },\n",
    "      {\n",
    "        \"grade2\": 1,\n",
    "        \"numericGrade2\": \"1\",\n",
    "        \"numericGrade2explain\": \"The article presents a neutral stance, discussing both the DNC's decision and the criticisms from Fox News anchors without showing clear sympathy or opposition.\"\n",
    "      },...\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "The first numeric grade should be either 0 or 1 based on whether or not the news article is about a labor strike or workplace protest.  If the article does not mention a labor strike or workplace protest, label the article with a 0. If the article mentions a labor strike or workplace protest, label the article with a 1. Then please identify one of the sentences in the article that has the word “strike”, AND differentiate whether or not this article is related to a labor strike, worker protest, union activity, employee activism OR if deemed a 0, provide a sentence from the provided article about an unrelated \"strike\" such as a drone strike. [Answer with a number in the 0-1 range, followed by a semi-colon, and then a brief motivation. Do not use quotation marks.]\n",
    "\n",
    "The second numeric grade should be between 0 and 2 based on its stance toward the protesting workers. A 0 indicates a lack of sympathy and is characterized by negative language used to describe the protestors or protest and/or a lack of justification of the workers’ rationale for protesting. If any negative outcomes or characteristics of the protest are described, including harms, disruptions, or inconveniences to customers and communities resulting from the protest, it should fall into this category. The following is an example of an article that is a 0:\n",
    "\n",
    "Workers at a private bus company that serves 8,000 riders a day in southern and eastern Brooklyn staged a sickout yesterday to protest their lack of a contract, complicating commuting just days before the Christmas holiday and catching city officials off guard.\n",
    "\n",
    "The job action, by 200 drivers, mechanics and other workers at the Command Bus Company of East New York, which receives city subsidies to operate six express and two local routes, affected only a tiny fraction of the transit systems daily passenger load of seven million.\n",
    "\n",
    "Nevertheless, Mayor Michael R. Bloomberg signed an order proclaiming a state of emergency and authorizing commuter vans licensed by the citys Taxi and Limousine Commission to pick up passengers along the affected routes.\n",
    "\n",
    "Aides to the mayor said he took the action for several reasons, including the cold weather and holiday traffic congestion.\n",
    "\n",
    "On Sunday and Monday, members of Local 1181 of the Amalgamated Transit Union, who work at Command Bus, and Local 1179, who work at Green Bus Lines of Jamaica, Queens, voted to authorize a strike.\n",
    "\n",
    "But city officials said on Monday that they believed that no job action would occur pending the outcome of a meeting yesterday between union officials and the citys commissioner of labor relations over a plan to take over Command, Green and five other private bus lines. Both sides continued to meet last night. Local 1179 officials said yesterday that they had decided not to strike at this time, preferring to see how talks developed.\n",
    "\n",
    "In Brooklyn neighborhoods including Canarsie, Starrett City, Gerritsen Beach and Mill Basin, riders faced long lines and confusion.\n",
    "\n",
    "The bus didnt show and everybody was waiting and didnt know what to do, said Geronimo Rodriguez, 21, who began his trip at Ocean Avenue and Avenue J. There were many students, maybe 100 people altogether.\n",
    "\n",
    "Mr. Rodriguez normally takes the B100, a popular line that travels east from Midwood to Mill Basin, where he works at the Little Israel grocery store.\n",
    "\n",
    "Yesterday, he had to walk 20 minutes to his job from the nearest stop on the B2 line. The delays, he said, doubled his normal travel time of one hour.\n",
    "\n",
    "Many commuters said that at first they thought the delays were caused by service changes.\n",
    "\n",
    "I was waiting for a half-hour, then 40 minutes, and nothing happened, said Luis Gines, 21, of Brighton Beach. Mr. Gines, who normally takes the B100, said he paid a car service 12 to get to his job as a kitchen manager at Not Just Chicken, a restaurant in Mill Basin.\n",
    "\n",
    "In 2002, the city and state announced that the Metropolitan Transportation Authority would take over seven private bus lines. The bus lines receive public subsidies of 200 million a year, and the city says that a public takeover would make bus service more efficient. Most of the bus workers would become employees of the authority, a state entity.\n",
    "\n",
    "However, negotiations have repeatedly foundered over pension liabilities, the fate of nonunion workers and the sale price of the companies bus depots and intangible assets like good will. The last contract for Command Bus workers expired in December 2002, and officials of Local 1181 have complained that their members have gone without raises while the talks drag on. The president of the local, Salvatore Battaglia, did not respond to a telephone message yesterday.\n",
    "\n",
    "The chairman of Command Bus, Jerome Cooper, said in a telephone interview that he was disappointed by the sickout but not surprised. Im sorry it came to this, said Mr. Cooper, who is also the chairman of Green and two other private bus companies. The only people that suffer are the people waiting on the street corners for buses.\n",
    "\n",
    "A 1 indicates neutrality. Both a justification of the workers’ rationale for protesting and the employer’s opposition to the protest is presented. Both consequences and benefits from the protest are presented. There should be no negative language used to describe either the protestors or the protest in this category. The following is an example of an article that is a 1:\n",
    "\n",
    "Two Detroit newspapers and the company that runs their joint business affairs have been ordered to reinstate 50 workers who were discharged in the course of labor protests while on strike, but have simultaneously been upheld in the firing of 35 others.\n",
    "\n",
    "That split decision was issued in Detroit on Friday by Richard Scully, an administrative law judge for the National Labor Relations Board. But it was not made public until yesterday, confirmed by both the Metropolitan Council of Newspaper Unions, a Detroit labor coalition, and Detroit Newspapers Inc., which runs the business operations of The Detroit News and The Detroit Free Press under a joint operating agreement.\n",
    "\n",
    "The ruling was the latest in an often bitter dispute that began in July 1995, when 2,370 employees struck Detroit Newspapers and the two publications over a variety of issues.\n",
    "\n",
    "The strike lasted 19 months, and before it was settled the 85 workers whose jobs were at stake in the decision made public yesterday were fired for a variety of protests at the newspapers offices or plants.\n",
    "\n",
    "Mr. Scully issued decisions on those workers -- some in news, others in production or distribution -- on a case-by-case basis, said Polk Laffoon IV, vice president for corporate relations at Knight-Ridder Inc., which owns The Free Press. The 50 employees reinstated, Mr. Scully held, must be taken back within 14 days.\n",
    "\n",
    "Susie Ellwood, a spokeswoman for Detroit Newspapers, said that we were obviously pleased that there were as many discharges upheld as there were but that its too soon for us to say what action we might take next.\n",
    "\n",
    "In a separate interview, Mr. Laffoon said We are reviewing the 155-page decision. In all likelihood, we will appeal the cases we lost.\n",
    "\n",
    "Shawn D. Ellis, a union spokesman who is one of the employees set for reinstatement under the ruling, said, What this community is hoping for is that the newspapers would take this opportunity once again to abide by the administrative law judge, return these workers to their jobs and abide by the previous rulings that have come down and begin returning real journalism back to Detroit.\n",
    "\n",
    "Mr. Ellis and Ms. Ellwood disagree on the number of workers already awaiting reinstatement as a result of the agreement by the six striking unions to end the walkout in February 1997.\n",
    "\n",
    "The newspapers said then that they would not discharge replacement workers but would hire the former strikers as jobs became available. Ms. Ellwood said 329 people were on the list of workers to be hired back Mr. Elliss count is twice as high.\n",
    "\n",
    "A 2 indicates sympathy and includes a justification of the workers' rationale for protesting. More positive opinions are presented than negative opinions from individuals who are interviewed. Positive outcomes or potential benefits of the protest are emphasized and there is no discussion of harms or inconveniences to customers and local communities resulting from the protest. The following is an example of an article that is a 2:\n",
    "\n",
    "Cond Nast, the venerable magazines parent company, agreed to a minimum salary of 60,000 by April 2023, the union said. The deal also covers workers at Ars Technica and Pitchfork.\n",
    "\n",
    "After more than two years of talks, an evening protest outside Anna Wintours townhouse in Greenwich Village and the threat of a strike, a group of union employees at The New Yorker and two other publications has reached a deal with their parent company, Cond Nast.\n",
    "\n",
    "The New Yorker Union and the unions representing two other Cond Nast publications, Ars Technica and Pitchfork, came to an agreement with the company on Wednesday after bargaining together for higher wages and improved health benefits, among other demands.\n",
    "\n",
    "These landmark agreements, which will go before members for a ratification vote in the coming weeks, will inaugurate a new era of equity, transparency and accountability at The New Yorker, Pitchfork, Ars Technica and Cond Nast at large, the unions for the three publications said in a joint statement.\n",
    "\n",
    "The three unions are affiliated with the NewsGuild, which also represents employees at The New York Times and other media organizations.\n",
    "\n",
    "During talks, The New Yorker Union demanded a base salary of 60,000, saying that some workers in the union made as little as 42,000 annually. The New Yorker Union is made up of fact checkers, copy editors and other editorial workers, but does not include the magazines staff writers.\n",
    "\n",
    "The deal with Cond Nast includes base pay of 55,000 for employees at all three unions, rising to 60,000 by April 2023. Under the agreement, many employees at the three publications will receive wage increases of at least 10 percent, the unions said in a statement.\n",
    "\n",
    "The agreement includes a cap on increases for health care costs and defined working hours. Contracts will also include a just cause provision stating that managers must provide specific reasons before disciplining or firing employees.\n",
    "\n",
    "I am elated that we have such a strong contract for our members now and such a strong contract to build on in future negotiations, Natalie Meade, a New Yorker fact checker and the unit chair of The New Yorker Union, said in an interview on Wednesday. She added that the union had been able to break the curse of stagnant wages at the publication.\n",
    "\n",
    "A spokeswoman for Cond Nast, which also publishes Vogue, Vanity Fair and Wired, among other publications, said the company was pleased to have reached an agreement with the unions. The new executive leadership team has implemented equitable compensation and inclusive benefits standards across our work force, she added. These standards are now reflected in our agreement with union employees.\n",
    "\n",
    "Roger J. Lynch joined the company as its chief executive in 2019. He leads Cond Nast with Ms. Wintour, the longtime editor of Vogue, who is also Cond Nasts worldwide chief content officer.\n",
    "\n",
    "The New Yorker Union formed in 2018. It was the first union in the 96-year history of the magazine that has the monocled dandy Eustace Tilley as its mascot.\n",
    "\n",
    "The New Yorker, which won six prizes at the National Magazine Awards last week, voluntarily recognized the union early on, but negotiations with Cond Nast leaders moved slowly.\n",
    "\n",
    "Last week, The New Yorker Union unveiled a website including the statement that it was on the verge of a strike. On June 8, roughly 100 people protested outside the home of Ms. Wintour, whose wide-ranging role at Cond Nast does not include oversight of The New Yorker. The New Yorker has been run by the editor David Remnick since 1998.\n",
    "\n",
    "[Answer with a number in the 0-2 range, followed by a semi-colon, and then a brief motivation. For instance: \"2; The text shows sympathy towards workers.\" Do not use quotation marks. If there is too little information to determine whether an article is a 0, 1, or 2, please mark it as a 1.]\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c7a5f",
   "metadata": {},
   "source": [
    "# 5. Calling the LLM and analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49984ef1",
   "metadata": {},
   "source": [
    "### 5.1 Call the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68c2b2",
   "metadata": {},
   "source": [
    "We will now write simple functions for calling the API and carry out our analysis request. We will also need to handle possible errors returned from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "776892ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def analyze_message(text, instruction, model = 'gpt-4o-mini', temperature=0.2):\n",
    "    print(f\"Analyzing message...\")\n",
    "    \n",
    "    response = None\n",
    "    tries = 0\n",
    "    failed = True\n",
    "    \n",
    "    while(failed):\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model = model, \n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": f\"'{instruction}'\"}, #The system instruction tells the bot how it is supposed to behave\n",
    "                        {\"role\": \"user\", \"content\": f\"'{text}'\"} #This provides the text to be analyzed.\n",
    "                    ]\n",
    "            )\n",
    "            failed = False\n",
    "\n",
    "        #Handle errors.\n",
    "        #If the API gets an error, perhaps because it is overwhelmed, we wait 10 seconds and then we try again. \n",
    "        # We do this 10 times, and then we give up.\n",
    "        except openai.error.APIError as e:\n",
    "            print(f\"OpenAI API returned an API Error: {e}\")\n",
    "            \n",
    "            if tries < 10:\n",
    "                print(f\"Caught an APIError: {e}. Waiting 10 seconds and then trying again...\")\n",
    "                failed = True\n",
    "                tries += 1\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"Caught an APIError: {e}. Too many exceptions. Giving up.\")\n",
    "                raise e\n",
    "                \n",
    "        except openai.error.ServiceUnavailableError as e:\n",
    "            print(f\"OpenAI API returned an ServiceUnavailable Error: {e}\")\n",
    "            \n",
    "            if tries < 10:\n",
    "                print(f\"Caught a ServiceUnavailable error: {e}. Waiting 10 seconds and then trying again...\")\n",
    "                failed = True\n",
    "                tries += 1\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"Caught a ServiceUnavailable error: {e}. Too many exceptions. Giving up.\")\n",
    "                raise e\n",
    "            \n",
    "        except openai.error.APIConnectionError as e:\n",
    "            print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "            pass\n",
    "        except openai.error.RateLimitError as e:\n",
    "            print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "            pass\n",
    "        \n",
    "        #If the text is too long, we truncate it and try again. Note that if you get this error, you probably want to chunk your texts.\n",
    "        except openai.error.InvalidRequestError as e:\n",
    "            #Shorten request text\n",
    "            print(f\"Received a InvalidRequestError. Request likely too long; cutting 10% of the text and trying again. {e}\")\n",
    "            time.sleep(5)\n",
    "            words = text.split()\n",
    "            num_words_to_remove = round(len(words) * 0.1)\n",
    "            remaining_words = words[:-num_words_to_remove]\n",
    "            text = ' '.join(remaining_words)\n",
    "            failed = True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Caught unhandled error.\")\n",
    "            pass\n",
    "            \n",
    "    result = ''\n",
    "    for choice in response.choices:\n",
    "        result += choice.message.content\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a805f",
   "metadata": {},
   "source": [
    "### 5.2 Parse response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99514df6",
   "metadata": {},
   "source": [
    "The LLM will return a text message. We need to parse this response so that we can use it for further analysis. The details of this function will depend on how you asked the API to respond in your instruction (see above). In our case, we asked the LLM to return a list of numbers, followed by a motivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a318a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(result):\n",
    "    #The LLMs at times surround their answers with quotation marks, even if you explicitly tell them not to. If so, we remove them her.\n",
    "    result = result.strip(\"'\\\"\") \n",
    "    try:\n",
    "        #We asked the LLM to start with a number, followed by a semi-colon, followed by the motivation. We assume this format in the response here.\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        #If we get an error, we here print the string that failed, to allow debugging.\n",
    "        print(result)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f4487",
   "metadata": {},
   "source": [
    "### 5.3 Run the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af547a85",
   "metadata": {},
   "source": [
    "This is the main loop of the code, where we call the LLM for each line in our data, and give it the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1eafcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model to what we actually want\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d561c8a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800\n"
     ]
    }
   ],
   "source": [
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2459c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we need to prepare the data and store it in a file for persistency\n",
    "filename = 'data.pkl'\n",
    "\n",
    "#These are the columns where we will store the analyzed data\n",
    "df['answers'] = [[] for _ in range(len(df))]\n",
    "\n",
    "df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "031cf55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 left to process. Processing: 29505\n",
      "Analyzing message...\n",
      "Analyzing message...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 12\u001b[0m, in \u001b[0;36manalyze_message\u001b[0;34m(text, instruction, model, temperature)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     13\u001b[0m         model \u001b[38;5;241m=\u001b[39m model, \n\u001b[1;32m     14\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     15\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     16\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \u001b[38;5;66;03m#The system instruction tells the bot how it is supposed to behave\u001b[39;00m\n\u001b[1;32m     17\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;66;03m#This provides the text to be analyzed.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m             ]\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:272\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    644\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    647\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    648\u001b[0m             {\n\u001b[1;32m    649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    669\u001b[0m             },\n\u001b[1;32m    670\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    671\u001b[0m         ),\n\u001b[1;32m    672\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    673\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    674\u001b[0m         ),\n\u001b[1;32m    675\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    676\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    677\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    678\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1085\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m )\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    854\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    855\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    856\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    857\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    858\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    859\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    938\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 150494 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#Analyze the specific line, chunk by chunk\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m---> 30\u001b[0m     result \u001b[38;5;241m=\u001b[39m analyze_message(chunk, instruction, model \u001b[38;5;241m=\u001b[39m MODEL)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#Parse the results, and put into dataframe\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     answers \u001b[38;5;241m=\u001b[39m parse_result(result)\n",
      "Cell \u001b[0;32mIn[47], line 25\u001b[0m, in \u001b[0;36manalyze_message\u001b[0;34m(text, instruction, model, temperature)\u001b[0m\n\u001b[1;32m     20\u001b[0m     failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#Handle errors.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#If the API gets an error, perhaps because it is overwhelmed, we wait 10 seconds and then we try again. \u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# We do this 10 times, and then we give up.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI API returned an API Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tries \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "#Main loop \n",
    "df = pd.read_pickle(filename)\n",
    "\n",
    "#If you want to limit the number of lines to analyze\n",
    "maximum_lines_to_analyze = 100000\n",
    "i = 0\n",
    "\n",
    "while(True):\n",
    "\n",
    "    #Find all unprocessed lines\n",
    "    # left = df.loc[df['result'].isna()]\n",
    "    left = df.loc[df['answers'].map(len)==0]\n",
    "    \n",
    "    #No lines left? Then we're done\n",
    "    if len(left)==0 or i>= maximum_lines_to_analyze:\n",
    "        print(\"All done!\")\n",
    "        break\n",
    "        \n",
    "    #Take a random line\n",
    "    line = left.sample()\n",
    "    index = line.index.values[0]\n",
    "    \n",
    "    print(f\"There are {len(left)} left to process. Processing: {index}\")\n",
    "    \n",
    "    #Wait for a bit, to not overload the API\n",
    "    time.sleep(WAIT_TIME)\n",
    "    \n",
    "    #Analyze the specific line, chunk by chunk\n",
    "    for chunk in line['text_chunks'].values[0]:\n",
    "        result = analyze_message(chunk, instruction, model = MODEL)\n",
    "\n",
    "        #Parse the results, and put into dataframe\n",
    "        answers = parse_result(result)\n",
    "        \n",
    "        # Skip if only one value is obtained\n",
    "        #if len(answer) == 1:  # Adjust this condition based on your definition of satisfactory result\n",
    "         #   print(f\"Skipping line {index} because only one value received.\")\n",
    "         #   continue\n",
    "            \n",
    "        #if len(motivation) == 1:  # Adjust this condition based on your definition of satisfactory result\n",
    "         #   print(f\"Skipping line {index} because only one value received.\")\n",
    "          #  continue\n",
    "\n",
    "        df.loc[index,'answers'].append(answers)\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "    #Save the result to persistent file\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7329e5d",
   "metadata": {},
   "source": [
    "### Post-analysis calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83ff6778",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save the result to persistent file\n",
    "df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28cc9bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data:\n",
      "       Unnamed: 0      X  year  \\\n",
      "0               1      1  2006   \n",
      "1               2      3  2006   \n",
      "2               3      4  2006   \n",
      "3               4      6  2006   \n",
      "4               5      8  2006   \n",
      "...           ...    ...   ...   \n",
      "38593       38594  45391  2022   \n",
      "38594       38595  45392  2022   \n",
      "38595       38596  45393  2022   \n",
      "38596       38597  45394  2022   \n",
      "38597       38598  45395  2022   \n",
      "\n",
      "                                                      V1  \\\n",
      "0      \"_AFFORDABLE WEAPON_ TESTS UNSUCCESSFUL, BUT H...   \n",
      "1      \"_At this point no new talks, no new negotiati...   \n",
      "2      \"_Can_t Imagine a Better Time_ - One on One Wi...   \n",
      "3      \"_Chaotic_ scene as carpenters walk Despite _p...   \n",
      "4      \"_I just didn_t think this was going to happen...   \n",
      "...                                                  ...   \n",
      "38593  \"You could be a joint homeowner _ with the sta...   \n",
      "38594  \"Your Friday Briefing_ U.S. Vaccine Mandate Bl...   \n",
      "38595  \"Your Monday Briefing.rtf.txt\"\"The New York Ti...   \n",
      "38596  \"Your Tuesday Briefing.rtf.txt\"\"The New York T...   \n",
      "38597  \"Zelenskyy_ Blinken & Austin Will Visit Kyiv S...   \n",
      "\n",
      "                                                 summary  \\\n",
      "0      Despite unsuccessful tests a weapon designed t...   \n",
      "1      Oct. 7--ST. MARYS -- Picketers hit the line in...   \n",
      "2      In his first interview as Sikorsky president J...   \n",
      "3      About 5000 unionized carpenters walked off the...   \n",
      "4      Retiree worried about benefits helps staff the...   \n",
      "...                                                  ...   \n",
      "38593  Jun 15 2022( CalMatters:Delivered by Newstex)M...   \n",
      "38594  Supreme Court justices dealt a blow to Bidens ...   \n",
      "38595  Top U.S. officials in Ukraine.Top U.S. officia...   \n",
      "38596  Omicrons effect on the pandemic.Will Omicron b...   \n",
      "38597  [20:00:37]PAMELA BROWN CNN HOST: I'm Pamela Br...   \n",
      "\n",
      "                         title  \\\n",
      "0              Vol. 19; No. 21   \n",
      "1      STATE AND REGIONAL NEWS   \n",
      "2               Vol. 40; No. 8   \n",
      "3                LOCAL; Pg. 07   \n",
      "4                     A; Pg. 1   \n",
      "...                        ...   \n",
      "38593                      NaN   \n",
      "38594                 BRIEFING   \n",
      "38595                 BRIEFING   \n",
      "38596                 BRIEFING   \n",
      "38597           NEWS; Domestic   \n",
      "\n",
      "                                                  header  \\\n",
      "0      \"_AFFORDABLE WEAPON_ TESTS UNSUCCESSFUL, BUT H...   \n",
      "1      \"_At this point no new talks, no new negotiati...   \n",
      "2      \"_Can_t Imagine a Better Time_ - One on One Wi...   \n",
      "3      \"_Chaotic_ scene as carpenters walk Despite _p...   \n",
      "4      \"_I just didn_t think this was going to happen...   \n",
      "...                                                  ...   \n",
      "38593  \"You could be a joint homeowner _ with the sta...   \n",
      "38594  \"Your Friday Briefing_ U.S. Vaccine Mandate Bl...   \n",
      "38595  \"Your Monday Briefing.rtf.txt\"\"The New York Ti...   \n",
      "38596  \"Your Tuesday Briefing.rtf.txt\"\"The New York T...   \n",
      "38597  \"Zelenskyy_ Blinken & Austin Will Visit Kyiv S...   \n",
      "\n",
      "                                source  \\\n",
      "0                      Inside the Navy   \n",
      "1                 The Lima News (Ohio)   \n",
      "2                         Rotor & Wing   \n",
      "3              Philadelphia Daily News   \n",
      "4      Lincoln Journal Star (Nebraska)   \n",
      "...                                ...   \n",
      "38593         Newstex Blogs CalMatters   \n",
      "38594              The New York Times    \n",
      "38595              The New York Times    \n",
      "38596              The New York Times    \n",
      "38597     CNN CNN NEWSROOM 8:00 PM EST   \n",
      "\n",
      "                                                  byline  \\\n",
      "0                                                    NaN   \n",
      "1                        Bob Blake, The Lima News, OhioB   \n",
      "2                                                    NaN   \n",
      "3                                         DAVE DAVIES, B   \n",
      "4      COLLEEN KENNEY, Lincoln Journal StarDateline:L...   \n",
      "...                                                  ...   \n",
      "38593                                      Emily HoevenB   \n",
      "38594  Melina DelkicHighlight:Supreme Court justices ...   \n",
      "38595  Natasha FrostHighlight:Top U.S. officials in U...   \n",
      "38596  Natasha FrostHighlight:Omicrons effect on the ...   \n",
      "38597  Pamela Brown, Matt Rivers, Scott McLean, Ed La...   \n",
      "\n",
      "                                        date  dayofweek  \\\n",
      "0                               May 29, 2006        NaN   \n",
      "1                   October 7, 2006 Saturday   Saturday   \n",
      "2                    August 15, 2006 Tuesday    Tuesday   \n",
      "3                        May 2, 2006 Tuesday    Tuesday   \n",
      "4      October 12, 2006 ThursdayCity Edition   Thursday   \n",
      "...                                      ...        ...   \n",
      "38593    June 15, 2022 Wednesday 2:35 PM EST  Wednesday   \n",
      "38594    January 13, 2022 Thursday 16:06 EST   Thursday   \n",
      "38595        April 25, 2022 Monday 00:50 EST     Monday   \n",
      "38596     January 18, 2022 Tuesday 00:44 EST    Tuesday   \n",
      "38597                April 23, 2022 Saturday   Saturday   \n",
      "\n",
      "                               copyrightco  length  \\\n",
      "0       2006 Inside Washington Publishers   1207.0   \n",
      "1                                      NaN   504.0   \n",
      "2           2006 Access Intelligence, LLC   2578.0   \n",
      "3       2006 Philadelphia Newspapers, LLC    380.0   \n",
      "4              2006 Lincoln Journal Star,    883.0   \n",
      "...                                    ...     ...   \n",
      "38593                    2022 Newstex LLC   3494.0   \n",
      "38594     2022 The New York Times Company   1388.0   \n",
      "38595     2022 The New York Times Company   1368.0   \n",
      "38596     2022 The New York Times Company   1504.0   \n",
      "38597             2022 Cable News Network   6110.0   \n",
      "\n",
      "                                                   photo  \\\n",
      "0                                                    NaN   \n",
      "1                                                    NaN   \n",
      "2                                                    NaN   \n",
      "3                                                    NaN   \n",
      "4         _I just didn_t think this was going to happen_   \n",
      "...                                                  ...   \n",
      "38593                                                NaN   \n",
      "38594  Your Friday Briefing_ U.S. Vaccine Mandate Blo...   \n",
      "38595                               Your Monday Briefing   \n",
      "38596                              Your Tuesday Briefing   \n",
      "38597                                                NaN   \n",
      "\n",
      "                                             text_chunks  \\\n",
      "0      [Despite unsuccessful tests a weapon designed ...   \n",
      "1      [Oct. 7--ST. MARYS -- Picketers hit the line i...   \n",
      "2      [In his first interview as Sikorsky president ...   \n",
      "3      [About 5000 unionized carpenters walked off th...   \n",
      "4      [Retiree worried about benefits helps staff th...   \n",
      "...                                                  ...   \n",
      "38593  [Jun 15 2022( CalMatters:Delivered by Newstex)...   \n",
      "38594  [Supreme Court justices dealt a blow to Bidens...   \n",
      "38595  [Top U.S. officials in Ukraine.Top U.S. offici...   \n",
      "38596  [Omicrons effect on the pandemic.Will Omicron ...   \n",
      "38597  [[20:00:37]PAMELA BROWN CNN HOST: I'm Pamela B...   \n",
      "\n",
      "                                                 answers  \n",
      "0      [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "1      [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "2      [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "3      [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "4      [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "...                                                  ...  \n",
      "38593  [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "38594  [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "38595  [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "38596  [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "38597  [[\\n  {\\n    \"grades\": [\\n      {\\n        \"gr...  \n",
      "\n",
      "[38597 rows x 16 columns]\n",
      "Filtered data saved to /Users/Nataliya/Desktop/Sentiment Analysis/filtered_data_24_10_21.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Pre-analysis test\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Specify the path to your .pkl file\n",
    "file_path = \"/Users/Nataliya/Desktop/Sentiment Analysis/data.pkl\"\n",
    "\n",
    "# Open the .pkl file in read-binary mode\n",
    "with open(file_path, 'rb') as file:\n",
    "    try:\n",
    "        # Load the contents of the .pkl file\n",
    "        data = pickle.load(file)\n",
    "        \n",
    "        # Filter data where 'answers' column is not empty\n",
    "        filtered_data = data[data['answers'].apply(len) > 0]\n",
    "        \n",
    "        # Print filtered data\n",
    "        print(\"Filtered Data:\")\n",
    "        print(filtered_data)\n",
    "        \n",
    "        # Save filtered data to an Excel file\n",
    "        excel_file_path = \"/Users/Nataliya/Desktop/Sentiment Analysis/filtered_data_24_10_21.xlsx\"  # Specify the path for the Excel file\n",
    "        filtered_data.to_excel(excel_file_path, index=False)\n",
    "        print(f\"Filtered data saved to {excel_file_path}\")\n",
    "    except EOFError:\n",
    "        # Handle the case where the file is not fully written yet\n",
    "        print(\"File is not fully written yet. Unable to load.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b52e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-analysis test\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Specify the path to your .pkl file\n",
    "file_path = \"/Users/Nataliya/Desktop/Sentiment Analysis/data.pkl\"\n",
    "\n",
    "# Open the .pkl file in read-binary mode\n",
    "with open(file_path, 'rb') as file:\n",
    "    try:\n",
    "        # Load the contents of the .pkl file\n",
    "        data = pickle.load(file)\n",
    "        \n",
    "        # Print or inspect the contents\n",
    "        print(data)  # Or any other way to inspect the data\n",
    "    except EOFError:\n",
    "        # Handle the case where the file is not fully written yet\n",
    "        print(\"File is not fully written yet. Unable to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405b4e7",
   "metadata": {},
   "source": [
    "Following the LLM analysis, we may need to do some minor calculations or modifications of the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06706e0d",
   "metadata": {},
   "source": [
    "For instance, we need to combine the values returned for the different chunks to a final complete values for the full text. This can be done in several ways, but the most straight-forward is to take the average values for each part. If the text only has one chunk, the result will be used without change. We will here leave the motivations as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9474ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom function to convert strings to numbers and calculate the mean\n",
    "def calculate_mean(lst):\n",
    "    # Convert each string to a number (float) and calculate the mean\n",
    "    numeric_values = [float(value) for value in lst if value.replace('.', '', 1).isdigit()]\n",
    "    return sum(numeric_values) / len(numeric_values) if len(numeric_values) > 0 else None\n",
    "\n",
    "# Apply the custom function to the 'strings' column\n",
    "df['mean_answers'] = df['answers'].apply(calculate_mean)\n",
    "\n",
    "# Display the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c6900",
   "metadata": {},
   "source": [
    "### Example result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d7973",
   "metadata": {},
   "source": [
    "We can now look at some examples of the result from the analysis and the associated motivation. \n",
    "\n",
    "For instance, we here look at the rating of Donald Trump's inaguration speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c22418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# albania = df.loc[df.filename == 'Albania_Berisha_Campaign_1.txt']\n",
    "# print(f\"\"\"Rating: {albania.answers.values[0]}. Motivation: '{albania.motivations.values[0][0]}'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ddaa9f",
   "metadata": {},
   "source": [
    "At face value, this motivation seems both reasonable and plausible. We will now turn to carry out a more in-depth validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51e981",
   "metadata": {},
   "source": [
    "# 6. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f439cc",
   "metadata": {},
   "source": [
    "Finally, we need to validate our results. Careful validation is essential to make sure that the models are measuring what we intend -- and that they do so without problematic biases. To validate our models, we can compare the outputs with established benchmarks, ground truth data, or expert evaluations to validate the effectiveness in achieving the desired analysis outcomes. Validation can furthermore help us fine-tune the model prompt to improve the results.\n",
    "\n",
    "A simple way of validating can be to output a random sample to an Excel file, and have human coders manually classifying the data to compare the results. To do so, the code below can be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7fa77b",
   "metadata": {},
   "source": [
    "### 6.1 Acquire validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to extract the data as excel for manual checking. This is included for illustration, however, we won't use this here.\n",
    "sample_size = 559\n",
    "sample = df.sample(sample_size).reset_index()\n",
    "sample['manual_classification'] = None\n",
    "sample.to_excel('/Users/Nataliya/Desktop/Sentiment Analysis/24-01-02-manual_validation_500articlesv4.xlsx')\n",
    "\n",
    "# # Now open the resulting file in Excel. Carry out manual classification and put result in the final column\n",
    "\n",
    "#manual_result = pd.read_excel('manual_validation_finished_NYT.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4e4a3",
   "metadata": {},
   "source": [
    "In the case of the populism example, however, the Global Populism Database already offers a large sample of manually classified datapoints that we can use for validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f27fc",
   "metadata": {},
   "source": [
    "We first need to make sure the data is in the right format for running simpledorff. Each line should be one coder response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load and clean the validation data. \n",
    "# val = pd.read_csv('./global-populism-dataset/gpd_v2_20220427.csv')\n",
    "# val = val[val['merging_variable'].notna()] \n",
    "# val = val[val['rubricgrade'].notna()] #The database contains some NaN values for the index; we remove these lines\n",
    "# val = val[['merging_variable','codernum','rubricgrade','averagerubric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We include only the lines that we've coded with the LLM\n",
    "# included = set(df.loc[~df['answer'].isna()].filename.values)\n",
    "# val = val.loc[(val['merging_variable'].isin(included)) &  (val['codernum']<=2) ]\n",
    "\n",
    "# #We compare our result with that of the average coder result\n",
    "# val = val.drop_duplicates(subset=['merging_variable'], keep='first')[['merging_variable','averagerubric']].rename(columns={'averagerubric':'answer'})\n",
    "# val['codernum'] = 'human'\n",
    "\n",
    "# #Fit our coded data into the same format to allow processing\n",
    "# df2 = df[['filename','answer','motivations']].dropna(subset=['answer']).rename(columns={'filename':'merging_variable'})\n",
    "# df2['codernum'] = 'llm'\n",
    "\n",
    "# #Combine the two datasets\n",
    "# validation_data = pd.concat([val,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd48a1d",
   "metadata": {},
   "source": [
    "### 6.2 Measure Krippendorf's Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8c6ea",
   "metadata": {},
   "source": [
    "To compare our data against the validation data, we can use Krippendorf's Alpha (see how-to guide for details.) We here use the simpledorff library to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpledorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpledorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./Sentiment Analysis/test.csv')\n",
    "\n",
    "#Calculate inter-coder reliability\n",
    "#Note that this uses the interval metric. If your variable is categorical, you need to remove the metric_fn parameter.\n",
    "KA = simpledorff.calculate_krippendorffs_alpha_for_df(test,experiment_col='eventid', annotator_col='codernum', class_col='answer')\n",
    "# metric_fn=simpledorff.metrics.interval_metric,\n",
    "\n",
    "print(f\"The resulting Krippendorf's Alpha is is {KA}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75763b36",
   "metadata": {},
   "source": [
    "This is a relatively high value for a first iteration of prompt development for a challenging concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2b604",
   "metadata": {},
   "source": [
    "### 6.3 Carry out iterative process of concept and prompt development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9c520",
   "metadata": {},
   "source": [
    "Having measured the disagreements between coders and LLM, we can now seek to try to understand the sources of the disagreement. This can be best thought of as a process of mutual learning through which we develop and operationalize a rigorous social scientific concept in the form of a prompt.\n",
    "\n",
    "We can here work with the coders, and comparing their notes to the motivations given by the LLM, focusing on the examples where the LLM and the human coders are (most) in disagreement. We may find that the prompt can be improved - or that our human coders were mistaken or biased. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca841fd",
   "metadata": {},
   "source": [
    "In our case, we do not have access to the coders, and we will simply show the process through which this form of work can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataframe that lists the level of disagreement between coders and LLM\n",
    "wrong = df2.merge(val, on='merging_variable')\n",
    "wrong['diff'] = abs(wrong['answer_x']-wrong['answer_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c128b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can save as CSV file to analyze results in Excel, or examine the results here.\n",
    "# display(wrong.sort_values(['diff']))\n",
    "wrong.sort_values(['diff']).to_csv('disagreements.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db561675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the cases where the LLM and human coders disagree the most is a speech by Berlusconi. The LLM does not think it is populist, but the human coders do.\n",
    "\n",
    "#The motivation given by the LLM is:\n",
    "wrong.loc[wrong['merging_variable']=='Italy_Berlusconi_Ribbon_2.txt'].motivations.values[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18f89f",
   "metadata": {},
   "source": [
    "Here follows the text, translated to English. \n",
    "\n",
    "Do you agree with the LLM or the huamn coders? If the latter, how do you think the prompt should be modified to improve the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Dear friends,\\n\\nIt is not easy to find the words to describe my, our state of mind at this moment. We are gathered here in Onna to celebrate the Liberation Day, a celebration that is both an honor and a commitment.\\n\\nAn honor: to commemorate a terrible massacre that took place right here in June 1944 when the Nazis, in retaliation, killed 17 citizens of Onna and then blew up the house where the bodies of those innocent victims were found.\\n\\nA commitment: what should inspire us is not to forget what happened here and to remember the horrors of totalitarianism and the suppression of 'freedom'.\\n\\nRight here, in Abruzzo, the legendary Maiella Brigade was born and operated, decorated with the Gold Medal for Military Valor. In December '43, 15 young people founded what would become the Maiella Brigade, which grew to 1,500 strong.\\n\\nIt is no coincidence that on this special day, the soldiers of the Honor Guard standing before us belong to the 33rd Artillery Regiment, the Abruzzesi unit that in 1943 on Cephalonia had the courage to resist the Nazis and sacrifice themselves – fighting – for the honor of our country.\\n\\nTo those patriots who fought for the redemption and rebirth of Italy, our admiration, gratitude, and recognition must always go.\\n\\nMost Italians today have not experienced what it means to be deprived of freedom. Only the elderly have a direct memory of totalitarianism, foreign occupation, and the war for the liberation of our homeland.\\n\\nFor many of us, it is a memory tied to our families, our parents, our grandparents, many of whom were protagonists or victims of those dramatic days. For me, it is the memory of years of separation from my father, forced to emigrate to avoid arrest, the memory of my mother's sacrifices, who alone had to support a large family during those difficult years. It is the memory of her courage, of her, like many others, traveling by train every day from a small town in the province of Como to work in Milan, and on one of those trains, risking her life but managing to save a Jewish woman from the clutches of a Nazi soldier destined for the extermination camps.\\n\\nThese are the memories, the examples with which we grew up – the memories of a generation of Italians who did not hesitate to choose freedom, even at the risk of their own safety and lives.\\n\\nOur country owes an inexhaustible debt to those many young people who sacrificed their lives during their most beautiful years to redeem the honor of the nation, out of fidelity to an oath, but above all for that great, splendid, and essential value which is freedom.\\n\\nWe owe the same debt of gratitude to all those other boys, Americans, English, French, Polish, from the many allied countries, who shed their blood in the Italian campaign. Without them, the sacrifice of our partisans would have risked being in vain.\\n\\nAnd with respect, we must remember today all the fallen, even those who fought on the wrong side, sincerely sacrificing their lives for their ideals and a lost cause.\\n\\nThis does not mean, of course, neutrality or indifference. We are – all free Italians are – on the side of those who fought for our freedom, for our dignity, and for the honor of our homeland.\\n\\nIn recent years, the history of the Resistance has been deepened and discussed. It is a good thing that it happened. The Resistance, along with the Risorgimento, is one of the founding values of our nation, a return to the tradition of freedom. And freedom is a right that comes before laws and the state because it is a natural right that belongs to us as human beings.\\n\\nHowever, a free nation does not need myths. As with the Risorgimento, we must also remember the dark pages of the civil war, even those in which those who fought on the right side made mistakes and took on blame.\\n\\nIt is an exercise in truth, in honesty, an exercise that makes the history of those who fought on the right side with selflessness and courage even more glorious.\\n\\nIt is the history of the many who fought in the Southern army, who, from Cephalonia onwards, redeemed the honor of the uniform with their blood.\\n\\nIt is the history of martyrs like Salvo D’Acquisto, who did not hesitate to sacrifice his life in exchange for other innocent lives.\\n\\nIt is the history of our soldiers interned in Germany who chose concentration camps rather than collaborating with the Nazis.\\n\\nIt is the history of the many who hid their fellow Jewish citizens, saving them from deportation.\\n\\nAbove all, it is the history of the many, countless unknown heroes who, with small or great acts of daily courage, contributed to the cause of freedom.\\n\\nEven the Church, I want to remember, played its part with true courage, to prevent odious concepts like race or religious differences from becoming reasons for persecution and death.\\n\\nSimilarly, we must remember the young Jews of the Jewish Brigade, who came from ghettos all over Europe, took up arms, and fought for freedom.\\n\\nAt that moment, many Italians of different faiths, cultures, and backgrounds came together to pursue the same great dream – the dream of freedom.\\n\\nAmong them were very different individuals and groups. Some thought only of freedom, some dreamed of establishing a different social and political order, some considered themselves bound by an oath of loyalty to the monarchy.\\n\\nBut they all managed to set aside their differences, even the most profound ones, to fight together. The communists and the Catholics, the socialists and the liberals, the actionists and the monarchists, faced with a common tragedy, each wrote a great page of our history. A page on which our Constitution is based, a page on which our freedom is based.\\n\\nIn the drafting of the Constitution, the wisdom of the political leaders of that time – De Gasperi and Togliatti, Ruini and Terracini, Nenni, Pacciardi, and Parri – managed to channel deep initial divisions towards a single objective.\\n\\nAlthough clearly the result of compromises, the republican Constitution achieved two noble and fundamental objectives: guaranteeing freedom and creating the conditions for democratic development in the country. It was not a small feat; in fact, it was the best compromise possible at the time.\\n\\nHowever, the goal of creating a \"common\" moral conscience for the nation was missed, perhaps premature for those times, so much so that the predominant value for everyone was anti-fascism, but not necessarily anti-totalitarianism. It was a product of history, a compromise useful to avoid the Cold War that vertically divided Italy from degenerating into a civil war with unpredictable outcomes. But the assumption of responsibility and the sense of the State that animated all the political leaders of that time remain a great lesson that would be unforgivable to forget.\\n\\nToday, 64 years after April 25, 1945, and twenty years after the fall of the Berlin Wall, our task, the task of all, is to finally build a unified national sentiment.\\n\\nWe must do it together, together, regardless of political affiliation, together, for a new beginning of our republican democracy, where all political parties recognize the greatest value, freedom, and debate in its name for the good and the interest of all.\\n\\nThe anniversary of the regained freedom is, therefore, an opportunity to reflect on the past, but also to reflect on the present and the future of Italy. If we can do it together from today onwards, we will have rendered a great service not to one\\n\\nWe have always rejected the idea that our adversary was our enemy. Our religion of freedom demanded it from us and still does. With the same spirit, I am convinced that the time has come for the Liberation Day to become the Day of Freedom, and for this commemoration to shed the character of opposition that revolutionary culture gave it, a character that still 'divides' rather than 'unites'.\\n\\nI say this with great serenity, without any intention of creating controversy. April 25 was the origin of a new season of democracy, and in democracy, the people's vote deserves absolute respect from everyone.\\n\\nAfter April 25, the people peacefully voted for the Republic, and the monarchy accepted the people's judgment.\\n\\nShortly after, on April 18, 1948, the people's choice was once again decisive for our country: with De Gasperi's victory, the Italian people recognized themselves in the Christian and liberal tradition of their history. The 1950s, always with the support of the popular vote, shaped Italy into a democratic, economic, and social reality. Italy became part of Europe and the West, played a role in promoting Atlantic unity and European unity, transforming from a rejected nation to a respected one.\\n\\nToday, our young people face other challenges: to defend the freedom conquered by their fathers and expand it even further, aware that without freedom, there can be no peace, justice, or well-being.\\n\\nSome of these challenges are global and see us engaged alongside free nations: the fight against terrorism, the fight against fanatic and repressive fundamentalism, the fight against racism because freedom, dignity, and peace are rights of every human being, 'everywhere' in the world.\\n\\nThat's why I want to remember the Italian soldiers engaged in peace missions abroad, especially those who have fallen in carrying out this noble mission. There is an ideal continuity between them and all the heroes, Italian and allied, who sacrificed their lives over 60 years ago to give us back freedom, security, and peace.\\n\\nToday, the teachings of our fathers take on a special value: this April 25 comes just after the great tragedy that struck this land of Abruzzo. Once again, facing the emergency and tragedy, Italians have shown their ability to unite, to overcome differences, demonstrating that they are a great and cohesive people, full of generosity, solidarity, and courage.\\n\\nLooking at the many Italians who have been engaged here in rescue and reconstruction efforts, I feel proud, once again, even more so, to be Italian and to lead this wonderful country.\\n\\nToday, Onna is the symbol of our Italy. The earthquake that destroyed it reminds us of the days when invaders destroyed it. Rebuilding it will mean repeating the gesture of its rebirth after Nazi violence.\\n\\nAnd it is precisely concerning the heroes of then and today that we all have a great responsibility: to set aside any controversy, to look at the interest of the nation, to safeguard the great heritage of freedom that we inherited from our fathers.\\n\\nTogether, we all have the responsibility and duty to build a future of prosperity, security, peace, and freedom for all.\\n\\nLong live Italy! Long live the Republic!\\n\\nLong live April 25, the celebration of all Italians who love freedom and want to remain free!\\n\\nLong live April 25, the celebration of regained freedom!\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
